* Kubernetes
  
** Kubernetes 中的基本概念

   
*** Master 
    
     是集群控制节点，每个 Kubernetes 集群里需要有一个 Master 节点来负责整个集群的管理和控制，基本上
     Kubernetes 所有的控制命令都是发给它.
     
     Master 节点上运行着以下一组关键进程：

     - Kubernetes API Server (kube-apiserver)，提供了 HTTP Rest 接口的关键服务进程，是 Kubernetes
       里所有资源的增、删、改、查等操作的唯一入口，也是集群控制的入口进程。
       
     - Kubernetes Controller Manager (kube-controller-manager), Kubernetes 里所有资源对象的
       自动化控制中心，可以理解为资源对象的 "大总管".
       
     - Kubernetes Scheduler (kube-scheduler)，负责资源调度 (Pod 调度) 的进程，相当于公交公司的 "调度室"。
       
     - etcd Server 保存所有资源对象的数据。


*** Node 节点
    
     一个节点是一个运行在 kubernetes 集群中的主机
     
     每个 Node 都会被 Master 分配一些工作负载 (Docker 容器)，当某个 Node 宕机时，其上的工作负载会被 Master
     自动转移到其他节点上去。
     
     kubernetes 的节点是集群中实际进行处理工作的点，节点可以是物理机或是虚机。每个节点上都运行有一些必要的服务以
     运行容器组，并且都可以通过主节点进行管理。必要的服务包括 Container Runtime (例如 Docker, rkt, runc 以
     及任何 OCI runtime-spec 实现)，kubelete 和代理服务 kube-proxy。
     
     关键进程包括:

     - kubelet: 负责 Pod 对应的容器的创建、启停等任务，同时与 Master 节点密切协作，实现集群管理的基本功能；
     - kube-proxy: 实现 Kubernetes Service 的通信与负载均衡机制的重要组件;
     - Docker Engine: Docker 引擎，负责本机的容器创建和管理工作；
       
     默认情况下，kubelet 会向 Master 注册自己，以实现 Node 节点在运行期间动态添加到 kubernetes 集群中。
     一旦 Node 被纳入集群管理范围，Kubelet 进程就会定时向 Master 节点汇报自身的情况，如操作系统、Docker 版本、
     机器的 CPU 和内存情况。

     Node 超过指定时间不上报信息，会被 Master 判定为 "失联"。



*** 容器组 (Pod)
    
     一个 Pod 对应于由若干容器组成的一个容器组，同个组内的容器共享一个存储卷 (volume)
     
     每个 Pod 都有一个特殊的被称为 "根容器" 的 Pause 容器。Pause 容器对应的镜像属于 Kubernetes
     平台的一部分，除了 Pause 容器，每个 Pod 还包含一个或多个紧密相关的用户业务容器。
     
       +----------------------------------------------+
       |                   Pod                        |
       |   +--------------------------------------+   |
       |   | gcr.io/google_containers/pause-amd64 |   |
       |   +--------------------------------------+   |
       |   +--------------------------------------+   |
       |   |      user container1 xxxImage        |   |
       |   +--------------------------------------+   |
       |   +--------------------------------------+   |
       |   |      user container2 xxxImage        |   |
       |   +--------------------------------------+   |
       |   +--------------------------------------+   |
       |   |      user container3 xxxImage        |   |
       |   +--------------------------------------+   |
       +----------------------------------------------+
         
     Kubernetes 设计出一个全新的 Pod 的概念并且 Pod 有这样特殊的组成结构的设计思想是

     - 在一组容器作为一个单元的情况下，难以对 "整体" 简单地进行判断及有效地进行处理。引入业务无关且
       不易死亡的 Pause 容器作为 Pod 的根容器，以它的状态代表整个容器组的状态，就简单、巧妙地解决
       了这个难题。
       
     - Pod 里的多个业务容器共享 Pause 容器的 IP，共享 Pause 容器挂接的 Volume，简化了密切关联的
       业务容器之间的通信问题，也很好地解决了它们之间的文件共享问题。
       

     Kubernetes 为每个 Pod 都分配了唯一的 IP 地址，称之为 Pod IP，一个 Pod 里的多个容器共享 Pod
     IP。Kubernetes 支持集群内任意两个 Pod 之间能通过 TCP/IP 直接通信。在 Kubernetes 里，一个 
     Pod 里的容器与另外主机上的 Pod 容器能够直接通信.
     
     Pod 支持两种类型：

     - 普通 Pod 
     - 静态 Pod (Static Pod)  -- 比较特殊，并不存放在 Kubernetes 的 etcd 存储里，而是存放在某个具体
                                Node 上的一个具体文件中，并且只在此 Node 上启动运行。
                                
     普通 Pod 一经创建，就会被放入到 etcd 中存储，随后会被 Master 调度到某个具体的 Node 上并进行绑定，
     然后该 Pod 被对应的 Node 上的 kubelet 进程实例化成一组相关的 Docker 容器并启动起来。默认情况下，
     当 Pod 里的某个容器停止时，Kubernetes 会自动检测到这个问题并重新启动这个 Pod (重启 Pod 里的所有
     容器)。如果 Pod 所在 Node 宕机，则会将这个 Node 上所有 Pod 重新调度到其他节点上。
     
     Pod IP + 其中容器的端口 = Pod 的一个 Endpoint
     
     Pod 可以对其能使用的服务器上的计算资源设置限额，支持 CPU 和 内存两种资源
     
     - CPU 以千分之一为最小单位，用 m 来表示，500m 表示 0.5 个 CPU 核;
     - 内存以字节数来作为单位；
       
     可以设置 Pod 的最小和最大计算资源限制

     - Requests: 该资源的最小申请量，系统必须满足
     - Limits: 该资源的最大允许使用量，不能被突破.

    
*** 容器组生命周期
    
     包含所有容器状态集合，包括容器组状态类型, 容器组生命周期, 时间, 重启策略, 以及 replication controllers.

    
*** Replication Controllers ( RC )
    
     主要负责控制在同一时间一起运行的 Pod 的数量。
    

*** Services 服务
    
     一个 Kubernetes 服务是容器组逻辑的高级抽象，同时也对外提供访问容器组的策略。


*** volumes 卷
    
     一个卷就是一个目录，容器对其有访问权限
 
    
*** labels 标签
    
     标签是用来连接一组对象的，比如容器组。标签可以被用来组织和选择子对象。
     

    
*** 接口权限 (accessing_the_api)
    
     端口，ip 地址和代理的防火墙规则。

    
*** web 界面
    
     用户可以通过 web 界面操作 Kubernetes.


*** 命令行操作
    
     kubecfg 命令。

  
** 容器技术基础
   
    Docker 容器这个听起来玄而又玄的概念，实际上是在创建容器进程时，指定了这个进程所需要的一组 Namespace 参数。
    这样，容器就只能看到当前 Namespace 所限定的资源、文件、设备、状态，或者配置。
    
    容器，其实是一种特殊的进程而已。
    
    跟真实存在的虚拟机不同，在使用 Docker 的时候，并没有一个真正的 "Docker 容器" 运行在宿主机里面。Docker 
    项目帮助用户启动的，还是原来的应用进程，只不过在创建这些进程时，Docker 为它们加上了各种各样的 Namespace
    参数。

    这些进程就会觉得自己是各自 PID Namespace 里的第 1 号进程，只能看到各自 Mount Namespace 里挂载的目录和
    文件，只能访问到各自 Network Namespace 里的网络设备，就仿佛运行在一个个 "容器" 里面。
    
    正确的 Docker 与 虚拟机 的架构对比图

    ./docker_vs_vm.png
    
    Docker 项目在这里扮演的角色更多的是旁路式的辅助和管理工作。
    
    敏捷 和 高性能 是容器相较于虚拟机最大的优势，也是其能在 PaaS 这种更细粒度的资源管理平台上大行其道的原因。
    
    容器是一个 “单进程” 模型。这就意味着，在一个容器中，没办法同时运行两个不同的应用，除非能事先找到一个公共的 PID = 1 
    的程序来充当两个不同应用的父进程，例如使用 systemd 或者 supervisord 来代替应用本身作为容器的启动进程。
    

*** 容器相较于虚拟机的不足

      最主要的问题是：隔离的不彻底。
      
      - 既然容器只是运行在宿主机上的一种特殊的进程，那么，多个容器之间使用的就还是同一个宿主机的操作系统内核。
        
        尽管可以在容器里面通过 Mount Namespace 单独挂载其他不同版本的操作系统文件。  --> 在 docker 中启动其他版本的操作系统，实际上是通过 Mount Namespace 来实现的。
        比如，CentOS 或是 Ubuntu，但这并不能改变共享宿主机内核的事实。这意味着，如果要在 Windows 宿主机上
        运行 Linux 容器，或者是在低版本的 Linux 宿主机上运行高版本的 Linux 容器，都是行不通的。
        

      - 在 Linux 内核中，有很多资源和对象是不能被 Namespace 化的，例如：时间。
        
        如果在容器中，调用 settimeofday 系统调用修改了时间，则整个宿主机的时间都会被随之修改。

        所以，在容器里部署应用的时候，什么能做，什么不能做，就是用户必须考虑的一个问题。
        
        
*** 容器所使用的系统资源限制

      Linux Cgroups 就是 Linux 内核中用来为进程设置资源限制的一个重要功能。
      
      Linux Cgroups 的全称是 Linux Control Group。其最主要的作用，就是限制一个进程组能够使用的资源
      上限，包括 CPU、内存、磁盘、网络带宽等。

      Linux Ggroup 给用户暴露出来的操作接口就是文件系统，简单理解，它就是一个子系统目录加上一组资源限制
      文件的组合。
      

**** 使用 Cgroup 对资源进行限制存在的问题

       /proc 文件系统问题。Linux 下的 /proc 目录存储的是记录当前内核运行状态的一系列特殊文件，但 /proc
       文件系统并不知道用户通过 Cgroups 给这个容器做了什么样的资源限制，因此用户在容器中通过 top 查看到的
       实际上是宿主机的资源使用情况。





       
       
*** 容器镜像

**** 容器中进程看到的文件系统
     
      容器中的应用进程应该看到一份完全独立的文件系统。
      
      Mount Namespace 跟其他 Namespace 的使用略有不同的地方：它对容器进程视图的改变，一定是伴随着挂载操作
      才能生效。
      
      使用 chroot 来切换进程的根目录。
      
      Docker 中会优先使用 pivot_root 系统调用，如果系统不支持，才会使用 chroot。
      

**** 容器镜像定义

      挂载在容器根目录上，用来为容器进程提供隔离后执行环境的文件系统，就是所谓的 "容器镜像"，即 rootfs (根文件系统)。
      
      rootfs 只是一个操作系统所包含的文件、配置和目录，并不包括操作系统内核。

      在同一台机器上的所有容器，都共享宿主机操作系统的内核。
      
      因为 rootfs 里打包的不只是应用，而是整个操作系统的文件和目录，也就意味着，应用以及它运行所需要的所有依赖，都被封装在
      一起，即使得容器具有一个最重要的特性 -- 一致性: 无论在本地、云端，还是在一台任何地方的机器上，用户只需要解压打包好的
      容器镜像，那么这个应用运行所需要的最完整的执行环境就会被重现出来。

      *这种深入到操作系统级别的运行环境一致性，打通了应用在本地开发和远端执行环境之间的壁垒*
      

**** 容器镜像的特性

      Docker 在镜像的设计中，引入了层 (Layer) 的概念，用户制作镜像的每一步操作，都会生成一个层，也就是一个增量 rootfs。
      
      这些 rootfs 的最下层，是来自 Docker 镜像的只读层。

      在只读层之上，是 Docker 自己添加的 init 层，用来存放被临时修改过的 /etc/hosts 等文件。
      
      而在 rootfs 的最上层是一个可读写层，它以 copy-on-write 的方式存放任何对只读层的修改，容器声明的 Volume 挂载点，
      也出现在这一层。


*** Docker 最核心的原理

     Docker 最核心的原理实际上就是为待创建的用户进程：

     1. 启用 Linux Namespace 配置；
     2. 设置指定的 Cgroups 参数；
     3. 切换进程的根目录 (Change root);

     可以通过执行 setns 系统调用，来让一个进程加入到另一个进程的指定命名空间中。


     
*** dockerinit 进程

      dockerinit 是 Docker 创建的一个容器初始化进程，会负责完成根目录的准备、挂载设备和目录、配置 hostname 等
      一系列需要在容器内进行的初始化操作。最后，会通过 execv() 系统调用，让应用进程取代自己，成为容器里的 PID = 1
      的进程。
      
**** Linux 的绑定挂载机制
     
        主要作用就是，允许将一个目录或是文件，而不是整个设备，挂载到一个指定的目录上。并且，此时在该挂载点上进行的任何
        操作，只是发生在被挂载的目录或者文件上，而原挂载点的内容则会被隐藏起来且不受影响。
        
        在 Linux 内核中，挂载绑定实际上是一个 inode 替换的过程。在 Linux 操作系统中，inode 可以理解为存放文件内容
        的 “对象”。而 dentry，也叫目录项，就是访问这个 inode 所使用的 “指针”
        

        mount --bind /home /test，会将 /home 挂载到 /test 上，其实相当于将 /test 的 dentry，重定向到了 /home
        的 inode。这样当我们修改 /test 目录时，实际修改的是 /home 目录的 inode。这也就是为何，一旦执行 umount 命令，
        /test 目录原先的内容就会恢复：因为修改真正发生在的，是 /home 目录里。
     

*** Docker Volume

      1. 容器里进程新建的文件，怎么才能让宿主机获取到？
      2. 宿主机上的文件和目录，怎么才能让容器里的进程访问到？

      Docker Volume 机制允许将宿主机上指定的目录或者文件，挂载到容器里面进行读取和修改操作。
      
      两种 Volume 声明方式：

      #+BEGIN_EXAMPLE
      $ docker run -v /test ...

      和 

      $ docker run -v /home:/test ...
      #+END_EXAMPLE
      
      两种声明方式本质是相同的：都是把一个宿主机的目录挂载到容器的 /test 目录。
      
      - 第一种并没有显式声明宿主机目录，那么 Docker 就会默认在宿主机中创建一个临时目录 /var/lib/docker/volumes/[VOLUME_ID]/_data,
        然后把它挂载到容器的 /test 目录上。

      - 第二种情况，Docker 就直接把宿主机的 /home 目录挂载到容器的 /test 目录下。


*** Dockerfile

      Dockerfile 的设计思想是，使用一些标准的原语, 描述我们所要构建的 Docker 镜像。并且这些原语，都是按照顺序处理的。
      
      Dockerfile 中的每个原语执行后，都会生成一个对应的镜像层。即使原语本身并没有明显地修改文件的操作 (比如，ENV 原语)，
      它对应的层也会存在。只不过在外界看来，这个层是空的。

      

** Kubernetes 集群的搭建与实践
   

*** 部署流程

      部署流程大致可以分为如下几步：

      - 在所有节点上安装 Docker 和 Kubeadm；
      - 部署 Kubernetes Master;
      - 部署容器网络插件；
      - 部署 kubernetes Worker；
      - 部署 Dashboard 可视化插件；
      - 部署容器存储插件；
        

*** 环境要求
    
      #+BEGIN_EXAMPLE
      - One or more machines running one of:
        + Ubuntu 16.04+
        + Debian 9
        + CentOS 7
        + RHEL 7
        + Fedora 25/26 (best-effort)
        + HypriotOS v1.0.1+Container Linux (tested with 1800.6.0)

      - 2 GB or more of RAM per machine (any less will leave little room for your apps)
      - 2 CPUs or more
      - Full network connectivity between all machines in the cluster (public or private network is fine)
      - Unique hostname, MAC address, and product_uuid for every node. See here for more details.
      - Certain ports are open on your machines. See here for more details.
      - Swap disabled. You MUST disable swap in order for the kubelet to work properly.
      #+END_EXAMPLE
        

*** 安装 Kubeadm
    
**** CentOS

      #+BEGIN_EXAMPLE
      cat <<EOF > /etc/yum.repos.d/kubernetes.repo
      [kubernetes]
      name=Kubernetes
      baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
      enabled=1
      gpgcheck=1
      repo_gpgcheck=1
      gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
      exclude=kube*
      EOF
      
      # Set SELinux in permissive mode (effectively disabling it)
      setenforce 0
      sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config
      
      yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes
      
      systemctl enable kubelet && systemctl start kubelet
      #+END_EXAMPLE
      
      CentOS 上可能会出现，由于 iptables 被绕过导致流量被错误路由的问题，应该确保 net.bridge.bridge-nf-call-iptables 的
      sysctl 配置被设置为 1
      
      #+BEGIN_EXAMPLE
      cat <<EOF >  /etc/sysctl.d/k8s.conf
      net.bridge.bridge-nf-call-ip6tables = 1
      net.bridge.bridge-nf-call-iptables = 1
      EOF
      sysctl --system
      #+END_EXAMPLE
      

**** Ubuntu

      #+BEGIN_EXAMPLE
      apt-get update && apt-get install -y apt-transport-https curl

      curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -

      cat <<EOF >/etc/apt/sources.list.d/kubernetes.list
      deb https://apt.kubernetes.io/ kubernetes-xenial main
      EOF

      apt-get update
      apt-get install -y kubelet kubeadm kubectl
      apt-mark hold kubelet kubeadm kubectl
      #+END_EXAMPLE

      



*** 安装 Docker
    
**** CentOS

      对于 CentOS 7 来说，CentOS-Extras 仓库中就已经包括了 docker，因此可以直接通过

      #+BEGIN_EXAMPLE
      $ yum install -y docker
      #+END_EXAMPLE
      
      来安装 docker。需要注意的一点是，一定要保证当前系统的内核版本高于 3.10.0-693，以
      保证 docker 能够正常使用 overlay2 storage driver。
      
      安装完成后，手动启动 docker daemon 服务

      #+BEGIN_EXAMPLE
      $ systemctl enable docker.service && systemctl start docker
      #+END_EXAMPLE
      
      安装好 docker 后，确认 docker 使用的是 overlay2 storage driver

      #+BEGIN_EXAMPLE
      $ docker info
      #+END_EXAMPLE
      
      若输出为

      #+BEGIN_EXAMPLE
      ...
      Storage Driver: overlay2
       Backing Filesystem: xfs
       Supports d_type: false
       Native Overlay Diff: true
      ...
      #+END_EXAMPLE
      
      则可以确认 docker 使用 storage driver 为 overlay2。

      
      
*** 关闭 swap

      #+BEGIN_EXAMPLE
      $ swapoff -a
      #+END_EXAMPLE

      
*** 新增 kubeadm.yaml 配置文件

      #+BEGIN_EXAMPLE
      apiVersion: kubeadm.k8s.io/v1alpha3
      kind: InitConfiguration
      controllerManagerExtraArgs:
        horizontal-pod-autoscaler-use-rest-clients: "true"
        horizontal-pod-autoscaler-sync-period: "10s"
        node-monitor-grace-period: "10s"
      apiServerExtraArgs:
        runtime-config: "api/all=true"
      kubernetesVersion: "stable-1.12"
      #+END_EXAMPLE
      
      其中，

      - kind: InitConfiguration 为 kubeadm v1alpha3 版本新引入的定义，在 v1alpha1 版本中
        需要定义为 MasterConfiguration；
        
      - stable-1.12 就是 kubeadm 部署的 Kubernetes 版本号，即: Kubernetes release 1.12
        的最新稳定版本，在当前环境中，即为 v1.12.2。当然，也可以直接指定这个版本，e.g.

        #+BEGIN_EXAMPLE
        kubernetesVersion: "v1.12.2"
        #+END_EXAMPLE
        
      - 为 kube-controller-manager 设置了

        #+BEGIN_EXAMPLE
        horizontal-pod-autoscaler-use-rest-clients: "true"
        #+END_EXAMPLE
        
        意味着，在将来部署的 kube-controller-manager 能够使用自定义资源 (Custom Metrics) 进行
        自动水平扩展。


      
*** 部署 Kubernetes Master
    
      只需要一条命令

      #+BEGIN_EXAMPLE
      $ kubeadm init --config kubeadm.yaml
      #+END_EXAMPLE
      

      部署成功的话，会输出

      #+BEGIN_EXAMPLE
      Your Kubernetes master has initialized successfully!

      To start using your cluster, you need to run the following as a regular user:
      
        mkdir -p $HOME/.kube
        sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
        sudo chown $(id -u):$(id -g) $HOME/.kube/config
      
      You should now deploy a pod network to the cluster.
      Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
        https://kubernetes.io/docs/concepts/cluster-administration/addons/
      
      You can now join any number of machines by running the following on each node
      as root:
      
        kubeadm join 10.11.28.234:6443 --token wynsdo.etmbh1txnpni5qst --discovery-token-ca-cert-hash sha256:2c87545af85b0ad111b69c9032c7815ad3aff89da6d7ab7640b950e4c62bfb6a
      #+END_EXAMPLE
      
      这样的信息
      
      其中 "kubeadm join ..." 命令就是用来给这个 Master 节点添加更多工作节点 (Worker) 的命令。
      
      此外，kubeadm 还会提示我们第一次使用 Kubernetes 集群所需要的配置命令：

      #+BEGIN_EXAMPLE
      mkdir -p $HOME/.kube
      sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
      sudo chown $(id -u):$(id -g) $HOME/.kube/config
      #+END_EXAMPLE
      
      需要这些配置命令的原因是：Kunernetes 集群默认需要加密方式访问。所以，这几条命令，就将刚刚部署生成
      的 Kubernetes 集群的安全配置文件，保存到当前用户的 .kube 目录下，kubectl 默认会使用这个目录下
      的授权信息访问 Kubernetes 集群。
      
      如果不这么做的话，每次都需要通过 export KUBECONFIG 环境变量告诉 kubectl 这个安全配置文件的位置。
      

**** 错误处理
     
      - 可能会报出 
        
        #+BEGIN_EXAMPLE
        "[preflight] running pre-flight checks [WARNING Service-Docker]: 
        docker service is not enabled, please run 'systemctl enable docker.service'"
        #+END_EXAMPLE
        
        的告警信息。

        可以通过执行

        #+BEGIN_EXAMPLE
        $ systemctl enable docker.service
        #+END_EXAMPLE
        
        来解决

        
*** 查看当前节点的状态
    
      #+BEGIN_EXAMPLE
      $ kubectl get nodes
      #+END_EXAMPLE
      
      Output:

      #+BEGIN_EXAMPLE
      NAME           STATUS     ROLES    AGE   VERSION
      10-11-28-234   NotReady   master   40m   v1.12.2
      #+END_EXAMPLE
      
      Note:

      能够看到，当前 Master 节点的状态是 NotReady。

      调试 Kubernetes 集群最重要的手段是用 kubectl describe 来查看这个节点对象的详细信息、状态和事件

      #+BEGIN_EXAMPLE
      $ kubectl describe node 10-11-28-234
      ...
      Conditions:
        Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
        ----             ------  -----------------                 ------------------                ------                       -------
        OutOfDisk        False   Wed, 07 Nov 2018 03:39:14 -0800   Wed, 07 Nov 2018 02:54:21 -0800   KubeletHasSufficientDisk     kubelet has sufficient disk space available
        MemoryPressure   False   Wed, 07 Nov 2018 03:39:14 -0800   Wed, 07 Nov 2018 02:54:21 -0800   KubeletHasSufficientMemory   kubelet has sufficient memory available
        DiskPressure     False   Wed, 07 Nov 2018 03:39:14 -0800   Wed, 07 Nov 2018 02:54:21 -0800   KubeletHasNoDiskPressure     kubelet has no disk pressure
        PIDPressure      False   Wed, 07 Nov 2018 03:39:14 -0800   Wed, 07 Nov 2018 02:54:21 -0800   KubeletHasSufficientPID      kubelet has sufficient PID available
        Ready            False   Wed, 07 Nov 2018 03:39:14 -0800   Wed, 07 Nov 2018 02:54:21 -0800   KubeletNotReady              runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
      Addresses:
      ...
      #+END_EXAMPLE
      
      可以看到，NodeNotReady 的原因是，尚未部署任何网络插件。
        
      
*** 部署网络插件
    
      Kubernetes 项目的设计理念是 - 一切皆容器。因此部署网络插件非常简单，只需要执行一句
      
      #+BEGIN_EXAMPLE
      $ kubectl apply
      #+END_EXAMPLE
      
      指令即可，以 Weave 为例

      #+BEGIN_EXAMPLE
      $ kubectl apply -f https://git.io/weave-kube-1.6
      #+END_EXAMPLE
      
      部署完成后，通过 kubectl get 重新检查 Pod 状态

      #+BEGIN_EXAMPLE
      [root@10-11-28-234 ~]# kubectl get pods -n kube-system
      NAME                                   READY   STATUS    RESTARTS   AGE
      coredns-576cbf47c7-4p4s4               1/1     Running   0          49m
      coredns-576cbf47c7-x6ksz               1/1     Running   0          49m
      etcd-10-11-28-234                      1/1     Running   0          48m
      kube-apiserver-10-11-28-234            1/1     Running   0          48m
      kube-controller-manager-10-11-28-234   1/1     Running   0          48m
      kube-proxy-r47l8                       1/1     Running   0          49m
      kube-scheduler-10-11-28-234            1/1     Running   0          48m
      weave-net-9kdjp                        1/2     Running   0          17s
      #+END_EXAMPLE
 
      
*** 部署 Kubernetes Worker

      部署 Kubernetes 的 Worker 节点只需要两步：

      - 在所有的 worker 节点上执行上面
        + 安装 Kubeadm
        + 安装 Docker
        + 关闭 swap
      
      - 执行部署 Master 节点时生成的 kubeadm join 命令
        
        #+BEGIN_EXAMPLE
        $ kubeadm join 10.11.28.234:6443 --token wynsdo.etmbh1txnpni5qst --discovery-token-ca-cert-hash sha256:2c87545af85b0ad111b69c9032c7815ad3aff89da6d7ab7640b950e4c62bfb6a
        #+END_EXAMPLE
        
      若部署成功，则会打印信息如

      #+BEGIN_EXAMPLE
      ...
      This node has joined the cluster:
      * Certificate signing request was sent to apiserver and a response was received.
      * The Kubelet was informed of the new secure connection details.
      
      Run 'kubectl get nodes' on the master to see this node join the cluster.
      ...
      #+END_EXAMPLE
      
      在 Master 节点上执行

      #+BEGIN_EXAMPLE
      $ kubectl get nodes
      #+END_EXAMPLE
      
      会输出所有节点信息

      #+BEGIN_EXAMPLE
      [root@10-11-28-234 ~]# kubectl get nodes
      NAME            STATUS   ROLES    AGE     VERSION
      10-11-133-193   Ready    <none>   4m41s   v1.12.2
      10-11-158-37    Ready    <none>   23m     v1.12.2
      10-11-169-178   Ready    <none>   11m     v1.12.2
      10-11-28-234    Ready    master   3h33m   v1.12.2
      #+END_EXAMPLE

      
*** 通过 Taint/Toleration 调整 Master 执行 Pod 的策略

      默认情况下，Master 节点是不允许运行用户 Pod 的。Kubernetes 做到这点，是依靠其
      Taint/Toleration 机制的。

      原理：一旦某个节点被加上了一个 "Taint"，即被打上了污点，那么所有 Pod 就都不能在
      这个节点上运行。

      除非有个别的 Pod 声明自己能够容忍这个污点，即声明了 Toleration，其才能在这个节点
      上运行。
      
**** 为节点打 Taint

      为节点打上 污点 的命令是：

      #+BEGIN_EXAMPLE
      $ kubectl taint nodes node1 foo=bar:NoSchedule
      #+END_EXAMPLE
      
      执行该命令后，node1 节点上就会增加一个键值对格式的 Taint，即 foo=bar:NoSchedule。
      其中值里面的 NoSchedule 表示这个 Taint 只会在调度新 Pod 时产生作用，而不会影响已经
      在 node1 上运行的 Pod，即使这些 Pod 没有声明 Toleration。
      

**** 为 Pod 声明 Toleration

      为 Pod 声明 Toleration 只要在 Pod 的 .yaml 文件中的 spec 部分，加入 toleration
      字段即可：

      #+BEGIN_EXAMPLE
      apiVersion: v1
      kind: Pod
      ...
      spec:
        tolerations:
        - key: "foo"
          operator: "Equal"
          value: "bar"
          effect: "NoSchedule"
      #+END_EXAMPLE
      
      这个 Toleration 的含义是，这个 Pod 能容忍所有键值对为 foo=bar 的 Taint，
      operator: "Equal" 表示 "等于" 的意思。
      

**** 查看 Master 节点的 Taint 配置

      #+BEGIN_EXAMPLE
      $ kubectl describe node 10-11-28-234
      #+END_EXAMPLE
      
      能够看到

      #+BEGIN_EXAMPLE
      ...
      Taints:             node-role.kubernetes.io/master:NoSchedule
      ...
      #+END_EXAMPLE
      
      Master 节点的 Taint 配置如上，其中键为

      node-role.kubernetes.io/master
      
      没有值。此时，若要为 Pod 添加 Toleration 声明，需要用 "Exists" 操作符来表明 "存在" 键。
      
      #+BEGIN_EXAMPLE
      apiVersion: v1
      kind: Pod
      ...
      spec:
        tolerations:
        - key: "foo"
          operator: "Exists"
          effect: "NoSchedule"
      #+END_EXAMPLE
      
      表明该 Pod 能容忍所有键为 foo 的 Taint。
      

**** 单节点 Kubernetes 集群

      若想要一个单节点的 Kubernetes 集群，删除 Master 节点的 Taint 才是正确的方法

      #+BEGIN_EXAMPLE
      $ kubectl taint nodes --all node-role.kubernetes.io/master-
      #+END_EXAMPLE
      
      在 node-role.kubernetes.io/master 键后面加上一个短线 "-" 表示移除所有以
      node-role.kubernetes.io/master 为键的 Taint
      

 
      
*** 部署 Dashboard 可视化插件
    
      TODO
      
*** 部署容器存储插件
    
      为了解决容器的无状态问题.
    
      容器的持久化存储，是用来保存容器存储状态的重要手段：存储插件会在容器里挂载一个基于
      网络或是其他机制的远程数据卷，使得在容器里创建的文件，实际上是保存在远程存储服务器
      上，或者以分布式的方式保存在多个节点上，而与当前宿主机没有任何绑定关系。
      
      这样在任意宿主机上启动新的容器，都可以请求挂载指定的持久化存储卷，从而访问到数据卷
      里的内容，即实现了持久化。
      
      TODO

      rook + Ceph

      
** 容器编排与 Kubernetes 核心特性剖析

   
** Kubernetes 开源社区与生态
 

   

* Kubernetes 权威指南

** 在 CentOS 环境中搭建单机版 Kubernetes 集群 
   
    操作系统环境：CentOS 7
    
*** 关闭 CentOS 自带的防火墙服务

     #+BEGIN_EXAMPLE
     $ systemctl disable firewalld
     $ systemctl stop firewalld
     #+END_EXAMPLE
     

*** 安装 etcd 和 Kubernetes

     #+BEGIN_EXAMPLE
     $ yum install -y etcd kubernetes
     #+END_EXAMPLE
     

*** 修改配置文件

     - Docker 配置文件

       修改 Docker 配置文件 /etc/sysconfig/docker 中的 OPTIONS 的内容设置为

       OPTIONS='--selinux-enabled=false --insecure-registry gcr.io'

       
     - Kubernetes apiserver 的配置文件

       修改 Kubernetes apiserver 的配置文件 /etc/kubernetes/apiserver，把 
       --admission_control 参数中的 ServiceAccount 删除。
       

*** 按顺序启动所有的服务

     #+BEGIN_EXAMPLE
     $ systemctl start etcd
     $ systemctl start docker
     $ systemctl start kube-apiserver
     $ systemctl start kube-controller-manager
     $ systemctl start kube-scheduler
     $ systemctl start kubelet
     $ systemctl start kube-proxy
     #+END_EXAMPLE
     
     Note:

     1. 启动 etcd 服务时，可能会报错，这主要是因为默认的 etcd 配置导致的问题，修改后可行的 
        etcd 配置文件 /etc/etcd/etcd.conf 示例为：

        #+BEGIN_EXAMPLE 
        # [member]
        ETCD_NAME=umstor                                                                        -- 节点名称
        ETCD_DATA_DIR=/var/lib/etcd                                                             -- 指定节点的数据存储目录
        ETCD_LISTEN_CLIENT_URLS=http://localhost:4001                                           -- 对外提供服务的地址，客户端会连接到这里和 etcd 进行交互
        ETCD_LISTEN_PEER_URLS=http://localhost:7001                                             -- 监听 URL, 用于与其他节点通信
        
        #[cluster]
        ETCD_INITIAL_ADVERTISE_PEER_URLS=http://localhost:7001                                  -- 该节点同伴监听地址，这个值会被告诉集群中的其他节点
        ETCD_ADVERTISE_CLIENT_URLS=http://localhost:4001                                        -- 对外公告的该节点客户端监听地址，这个值会告诉集群中其他节点
        ETCD_INITIAL_CLUSTER_STATE=new                                                          -- 新建集群的时候，这个值为 new；假如已经存在的集群，这个值为 existing
        ETCD_INITIAL_CLUSTER_TOKEN=umstor                                                       -- 创建集群的 token，这个值每个集群保持唯一。这样的话，如果你要重新创建集群，即使配置和之前一样，也会再次生成新的集群和节点 uuid；否则会导致多个集群之间的冲突，造成未知的错误
        ETCD_INITIAL_CLUSTER=umstor=http://localhost:7001                                       -- 集群中所有节点的信息，这里的 umstor 是节点的 name 所指定的名字，后面的 ip 地址是 initial-advertise-peer-urls 指定的值
        #ETCD_INITIAL_CLUSTER="etcd1=http://k8s_master_ip1,etcd2=http://k8s_master_ip2:2380"
        #+END_EXAMPLE
        

     2. 启动 kube-apiserver 服务时，可能会报错，通常是因为 /etc/kubernetes/apiserver 
        配置文件中的配置有问题，在我的环境中，因为 etcd 对外提供服务的地址是 localhost:4001
        而 apiserver 配置文件中 KUBE_ETCD_SERVERS 配置项默认为 127.0.0.1:2378，因此
        需要将其端口号修改为 4001.
     

   至此，一个单机版的 Kubernetes 集群环境就安装启动完成了。
   

   
** 在搭建的单机版 Kubernetes 集群中启动 mysql
   
*** 启动 mysql RC Pod
   
      为 mysql 服务创建 RC 定义文件: mysql-rc.yaml
  
      #+BEGIN_EXAMPLE
      apiVersion: v1
      kind: ReplicationController                  -- 副本控制器 RC
      metadata:
        name: mysql                                -- RC 的名称，全局唯一
      spec:
        replicas: 1                                -- Pod 副本期待数量
        selector:
          app: mysql                               -- 符合目标的 Pod 拥有此标签
        template:                                  -- 根据此模板创建 Pod 的副本 (实例)
          metadata:
            labels:
              app: mysql                           -- Pod 副本拥有的标签，对应 RC 的 selector
          spec:
            containers:                            -- Pod 内容器的定义部分
            - name: mysql                          -- 容器的名称
              image : mysql:5.7                    -- 容器对应的 Docker Image
              imagePullPolicy: IfNotPresent
              ports:
              - containerPort: 3306                -- 容器暴露的端口号
              env:                                 -- 注入到容器内的环境变量
              - name: MYSQL_ROOT_PASSWORD
                value: "123456"
      #+END_EXAMPLE
      
      - kind 用来表示此资源对象的类型
      - spec 一节中是 RC 的相关属性定义，比如 spec.selector 是 RC 的 Pod 标签 (Label)
        选择器，即监控和管理拥有这些标签的 Pod 实例，确保当前集群上始终有且仅有 replicas 个
        Pod 实例在运行。
      - 当集群中的 Pod 数量少于 spec.replicas 定义的数量时，RC 会根据 spec.template 一
        节中定义的 Pod 模板来生成一个新的 Pod 实例。
        
  
      将上述 mysql-rc.yaml 发布到 Kubernetes 集群中，需要在 Master 节点上执行
  
      #+BEGIN_EXAMPLE
      $ kubectl create -f mysql-rc.yaml
      #+END_EXAMPLE
      
      在上述创建 Pod 的过程中，可能会报出
  
      #+BEGIN_EXAMPLE
      Error syncing pod, skipping: failed to "StartContainer" for "POD" with 
      ErrImagePull: "image pull failed for registry.access.redhat.com/rhel7/pod-infrastructure:latest, 
      this may be because there are no credentials on this request. details: 
      (open /etc/docker/certs.d/registry.access.redhat.com/redhat-ca.crt: no such file or directory)"
      #+END_EXAMPLE
      
      的错误，大致意思就是：未能通过ErrImagePull为“POD”启动“StartContainer”：
      “对于registry.access.redhat.com/rhel7/pod-infrastructure:latest，镜像拉出失败，这可能是因为此请求上没有证书 
  
      检查发现：/etc/docker/certs.d/registry.access.redhat.com/redhat-ca.crt这个目录中是一个软连接
      
      看报错信息和 rhsm 有关，全称为 RedHat Subscription Manager，安装该服务
  
      #+BEGIN_EXAMPLE
      $ yum install *rhsm*
      #+END_EXAMPLE
      
      再次创建 Pod 发现并没有解决问题
      
      再找解决方法
  
      #+BEGIN_EXAMPLE
      wget http://mirror.centos.org/centos/7/os/x86_64/Packages/python-rhsm-certificates-1.19.10-1.el7_4.x86_64.rpm
      rpm2cpio python-rhsm-certificates-1.19.10-1.el7_4.x86_64.rpm | cpio -iv --to-stdout ./etc/rhsm/ca/redhat-uep.pem | tee /etc/rhsm/ca/redhat-uep.pem
      #+END_EXAMPLE
      
      这一次发现在 /etc/rhsm/ca 目录下生成了 redhat-uep.pem 文件
      
      创建 Pod 成功

    
      
*** 创建一个关联的 Kubernetes Service
    
      yaml 定义文件如下

      #+BEGIN_EXAMPLE
      apiVersion: v1
      kind: Service          -- 表明是 Kubernetes Service
      metadata:
        name: mysql          -- Service 全局唯一名称
      spec:
        ports:
          - port: 3306       -- Service 提供服务的端口号
        selector:            -- Service 对应的 Pod 拥有这里定义的标签
          app: mysql
      #+END_EXAMPLE
      
      - spec.selector 确定了哪些 Pod 副本对应到本服务

      执行如下命令创建 Service 对象

      #+BEGIN_EXAMPLE
      $ kubectl create -f mysql-svc.yaml
      #+END_EXAMPLE
      

** 在搭建的单机版 Kubernetes 集群中启动 tomcat 应用
   
*** 启动 tomcat RC 实例
    
      创建 myweb-rc.yaml 文件

      #+BEGIN_EXAMPLE
      apiVersion: v1
      kind: ReplicationController
      metadata:
        name: myweb
      spec:
        replicas: 1
        selector:
          app: myweb
        template:
          metadata:
            labels:
              app: myweb
          spec:
            containers:
            - name: myweb
              image: kubeguide/tomcat-app:v1
              ports:
              - containerPort: 8080
              env:
              - name: MYSQL_SERVICE_HOST
                value: '10.254.45.56'
              - name: MYSQL_SERVICE_PORT
                value: '3306'
              - name: MYSQL_ROOT_PASSWORD
                value: "123456"
      #+END_EXAMPLE
      
      - MYSQL_SERVICE_HOST 环境变量对应的值为 mysql 服务的 cluster ip
        
      通过如下命令启动 myweb Pod 实例

      #+BEGIN_EXAMPLE
      $ kubectl create -f myweb.yaml
      #+END_EXAMPLE
      

*** 启动 tomcat 应用 Service 实例
    
      准备 yaml 配置文件

      #+BEGIN_EXAMPLE
      apiVersion: v1
      kind: Service
      metadata:
        name: myweb
      spec:
        type: NodePort
        ports:
          - port: 8080
            nodePort: 30001
        selector:
          app: myweb
      #+END_EXAMPLE
      
      Note

      type=NodePort 和 nodePort = 30001 的两个属性，表明此 Service 开启了 NodePort 方式
      的外网访问模式，在 Kubernetes 集群以外，比如在本机的浏览器里，可以通过 30001 端口访问
      myweb 
      
      通过如下命令创建 Service 实例
      
      #+BEGIN_EXAMPLE
      $ kubectl create -f myweb-svc.yaml
      #+END_EXAMPLE
      

      
  至此，正常情况下，应该就可以在本地机器上，通过在浏览器中输入
  http://192.168.2.16:30001/demo/ (假设 Kubernetes 集群搭建在 192.168.2.16
  服务器上) 来查看结果了。

  但测试时，发现，会报出

  #+BEGIN_EXAMPLE
  Error:com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link 
  failure The last packet sent successfully to the server was 0 milliseconds ago. The 
  driver has not received any packets from the server.
  #+END_EXAMPLE
  
  的错误。谷歌之发现，这种错误通常是以为前端应用无法连接到 mysql 导致的。
  
  通过执行

  #+BEGIN_EXAMPLE
  $ kubectl exec -it myweb-xxxx -- /bin/bash
  #+END_EXAMPLE
  
  进入到容器中进行查看，发现 webapp/demo/index.html 文件中的内容为

  #+BEGIN_EXAMPLE
  ...
  String ip=System.getenv("MYSQL_SERVICE_HOST");
  String port=System.getenv("MYSQL_SERVICE_PORT");
  ip=(ip==null)?"localhost":ip;
  port=(port==null)?"3306":port;
  ...
  #+END_EXAMPLE
  
  可以看到，IP 是通过 MYSQL_SERVICE_HOST 环境变量获取的，查看 MYSQL_SERVICE_HOST 环境变量内容

  #+BEGIN_EXAMPLE
  echo $MYSQL_SERVICE_HOST
  mysql
  #+END_EXAMPLE
  
  发现环境变量名为 mysql，显然是有问题的，于是在 myweb-rc.yaml 文件中，将该环境变量的值修改为 mysql
  Service 的 cluster ip，再次尝试，发现，还是报错
  
  通过查看 k8s 权威教程的官方博客了解到，这个可能是因为 mysql 版本的问题引起的

  http://blog.leanote.com/post/w2w.wzz@foxmail.com/k8s-%E6%9D%83%E5%A8%81%E6%95%99%E7%A8%8B%E7%AC%AC%E4%B8%80%E7%AB%A0demo%EF%BC%88Bug%EF%BC%89
  
  因此，修改 mysql-rc.yaml 文件中的 mysql 容器镜像为 mysql:5.7, 重启创建 mysql Pod 实例和 Service 实例，
  再次在本地浏览器中访问 http://192.168.2.16:30001/demo/ ，成功了。
  


* Reference

[1] https://kubernetes.io/docs/setup/independent/install-kubeadm/
